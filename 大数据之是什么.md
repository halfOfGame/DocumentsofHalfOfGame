# 大数据之是什么

[toc]

## Hadoop

### 是什么

​	**Hadoop**是由Apache基金会所开发的分布式系统基础架构。广义上指的是Hadoop生态圈。

### 作用

​	解决数据的存储和海量数据的分析计算问题。

### 组成

<img src="C:\Users\halfofgame\Documents\MarkdownImages\Hadoop组成.png" alt="Hadoop组成"  />



## HDFS

### 是什么

**HDFS**是一个分布式文件系统。

### 作用

- 用于存储文件，通过目录树来定位文件。
- 其次，它是分布式的，由很多服务器联合起来实现其功能，集群的服务器有各自的职责。

### 组成

![HDFS架构1](C:\Users\halfofgame\Documents\MarkdownImages\HDFS架构1.png)

![HDFS架构2](C:\Users\halfofgame\Documents\MarkdownImages\HDFS架构2.png)

### 工作流程

- NN和2NN工作机制

  1. 第一阶段：NameNode启动

     （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。

     （2）客户端对元数据进行增删改的请求。

     （3）NameNode记录操作日志，更新滚动日志。

     （4）NameNode在内存中对数据进行增删改。

  2. 第二阶段：Secondary NameNode工作

     （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。

     （2）Secondary NameNode请求执行CheckPoint。

     （3）NameNode滚动正在写的Edits日志。

     （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。

     （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。

     （6）生成新的镜像文件fsimage.chkpoint。

     （7）拷贝fsimage.chkpoint到NameNode。

     （8）NameNode将fsimage.chkpoint重新命名成fsimage



![NameNode工作机制](C:\Users\halfofgame\Documents\MarkdownImages\NameNode工作机制.png)

![NameNode工作机制详解](C:\Users\halfofgame\Documents\MarkdownImages\NameNode工作机制详解.png)



## MapReduce

### 是什么

**MapReduce**是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。

### 作用

**MapReduce**的核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。

### 一些概念

- **Job(作业)** ：一个MR程序称为一个Job
- **MRAppMaster**（MR任务的主节点）:  一个Job在运行时，会先启动一个进程，这个进程为 MRAppMaster。负责Job中执行状态的监控，容错，和RM申请资源，提交Task等！
- **Task**(任务)：Task是一个进程！负责某项计算！
- **Map**(Map阶段): Map是MapReduce程序运行的第一个阶段！Map阶段的目的是将输入的数据，进行切分。将一个大数据，切分为若干小部分！切分后，每个部分称为1片(split)，每片数据会交给一个Task（进程）进行计算！Task负责是Map阶段程序的计算，称为MapTask!在一个MR程序的Map阶段，会启动N（取决于切片数）个MapTask。每个MapTask是并行运行！
- **Reduce**(Reduce阶段)：Reduce是MapReduce程序运行的第二个阶段(最后一个阶段)！Reduce阶段的目的是将Map阶段，每个MapTask计算后的结果进行合并汇总！得到最终结果！Reduce阶段是可选的！Task负责是Reduce阶段程序的计算，称为ReduceTask!一个Job可以通过设置，启动N个ReduceTask，这些ReduceTask也是并行运行！每个ReduceTask最终都会产生一个结果！

### 工作流程与亿些细节

#### MapReduce中常用的组件

- **Mapper**:  map阶段核心的处理逻辑
- **Reducer**:  reduce阶段核心的处理逻辑
- **InputFormat**: MapReduce的输入格式，MR程序必须指定一个输入目录，一个输出目录！
	如果是普通文件，可以使用FileInputFormat.
	如果是SequeceFile（hadoop提供的一种文件格式），可以使用SequnceFileInputFormat.
	如果处理的数据在数据库中，需要使用DBInputFormat
	
- **RecordReader**:  记录读取器
				RecordReader负责从输入格式中，读取数据，读取后封装为一组记录(k-v)!
- **OutPutFormat**: MapReduce的输出格式
			OutPutFormat代表MR处理后的结果，要以什么样的文件格式写出！
	将结果写出到一个普通文件中，可以使用FileOutputFormat！
	将结果写出到数据库中，可以使用DBOutPutFormat！
	将结果写出到SequeceFile中，可以使用SequnceFileOutputFormat
- **RecordWriter**: 记录写出器
			RecordWriter将处理的结果以什么样的格式，写出到输出文件中！
- **Partitioner**: 分区器
		分区器，负责在Mapper将数据写出时，将keyout-valueout，为每组keyout-valueout打上标记，进行分区！
	目的： 一个ReduceTask只会处理一个分区的数据！

#### MapReduce的数据流程

1. InputFormat调用RecordReader，从输入目录的文件中，读取一组数据，封装为keyin-valuein对象
2. 将封装好的key-value，交给Mapper.map()------>将处理的结果写出 keyout-valueout
3. ReduceTask启动Reducer，使用Reducer.reduce()处理Mapper写出的keyout-valueout，
4. OutPutFormat调用RecordWriter，将Reducer处理后的keyout-valueout写出到文件

#### MapReduce运行流程示例

- **需求**：
  统计/hello目录中每个文件的单词数量，
  a-p开头的单词放入到一个结果文件中，
  q-z开头的单词放入到一个结果文件中。
  例如： 
  /hello/a.txt   200M
  hello,hi,hadoop
  hive,hadoop,hive,
  zoo,spark,wow
  zoo,spark,wow
  ...

  /hello/b.txt    100m
  hello,hi,hadoop
  zoo,spark,wow
   ...

- 过程

- Map阶段(运行MapTask，将一个大的任务切分为若干小任务，处理输出阶段性的结果)
  1. 切片(切分数据)
     /hello/a.txt   200M
     /hello/b.txt    100m
     默认的切分策略是以文件为单位，以文件的块大小(128M)为片大小进行切片！
     split0:/hello/a.txt,0-128M
     split1: /hello/a.txt,128M-200M
     split2: /hello/b.txt,0M-100M
  2. 运行MapTask（进程），每个MapTask负责一片数据
     split0:/hello/a.txt,0-128M--------MapTask1
     split1: /hello/a.txt,128M-200M--------MapTask2
     split2: /hello/b.txt,0M-100M--------MapTask3
  3. 读取数据阶段
     在MR中，所有的数据必须封装为key-value
     MapTask1,2,3都会初始化一个InputFormat（默认TextInputFormat），每个InputFormat对象负责创建一个RecordReader(LineRecordReader)对象，
  RecordReader负责从每个切片的数据中读取数据，封装为key-value.
     LineRecordReader: 将文件中的每一行封装为一个key（offset）-value(当前行的内容)
  举例：
  hello,hi,hadoop----->(0,hello,hi,hadoop)
  hive,hadoop,hive----->(20,hive,hadoop,hive)
  zoo,spark,wow----->(30,zoo,spark,wow)
  zoo,spark,wow----->(40,zoo,spark,wow)
  5. 进入Mapper的map()阶段
     	map()是Map阶段的核心处理逻辑！ 单词统计! map()会循环调用，对输入的每个Key-value都进行处理！
     输入：(0,hello,hi,hadoop)
     输出：(hello,1),(hi,1),(hadoop,1) 
     
     输入：(20,hive,hadoop,hive)
     输出：(hive,1),(hadoop,1),(hive,1) 
     
     输入：(30,zoo,spark,wow)
     输出：(zoo,1),(spark,1),(wow,1) 
        	
     输入：(40,zoo,spark,wow)
     输出：(zoo,1),(spark,1),(wow,1) 

  5. 目前，我们需要启动两个ReduceTask,生成两个结果文件，需要将MapTask输出的记录进行分区(分组，分类)
     在Mapper输出后，调用Partitioner，对Mapper输出的key-value进行分区，分区后也会排序（默认字典顺序排序）
     分区规则：
     a-p开头的单词放入到一个区
     q-z开头的单词放入到另一个区
     MapTask1:
     0号区：  (hadoop,1)，(hadoop,1)，(hello,1),(hi,1),(hive,1),(hive,1)
     1号区：  (spark,1),(spark,1),(wow,1) ，(wow,1),(zoo,1)(zoo,1)

     MapTask2:
     0号区：  。。。
     1号区： ...

     MapTask3:
     0号区：   (hadoop,1),(hello,1),(hi,1),
     1号区： (spark,1),(wow,1),(zoo,1)

- Reduce阶段
  1. copy
     ReduceTask启动后，会启动shuffle线程，从MapTask中拷贝相应分区的数据！
     ReduceTask1: 只负责0号区
     将三个MapTask，生成的0号区数据全部拷贝到ReduceTask所在的机器！
     (hadoop,1)，(hadoop,1)，(hello,1),(hi,1),(hive,1),(hive,1)
     (hadoop,1),(hello,1),(hi,1),
     		
     		
     ReduceTask2: 只负责1号区
     将三个MapTask，生成的1号区数据全部拷贝到ReduceTask所在的机器！
     (spark,1),(spark,1),(wow,1) ，(wow,1),(zoo,1)(zoo,1)
     (spark,1),(wow,1),(zoo,1)
     
  2. sort
     ReduceTask1:	只负责0号区进行排序：
     	(hadoop,1)，(hadoop,1)，(hadoop,1),(hello,1),(hello,1),(hi,1),(hi,1),(hive,1),(hive,1)
     
     ReduceTask2: 只负责1号区进行排序：
     	(spark,1),(spark,1),(spark,1),(wow,1) ，(wow,1),(wow,1),(zoo,1),(zoo,1)(zoo,1)
     
  3. reduce
     	ReduceTask1---->Reducer----->reduce(一次读入一组数据)
      何为一组数据： key相同的为一组数据
      输入： (hadoop,1)，(hadoop,1)，(hadoop,1)
      输出：   (hadoop,3)
      
      输入： (hello,1),(hello,1)
      输出：   (hello,2)
      
      输入： (hi,1),(hi,1)
      输出：  (hi,2)
      
      输入：(hive,1),(hive,1)
      输出： （hive,2）
      
      ReduceTask2---->Reducer----->reduce(一次读入一组数据)
      
      输入： (spark,1),(spark,1),(spark,1)
      输出：   (spark,3)
      
      输入： (wow,1) ，(wow,1),(wow,1)
      输出：   (wow,3)
      
      输入：(zoo,1),(zoo,1)(zoo,1)
      输出：   (zoo,3)
4. 调用OutPutFormat中的RecordWriter将Reducer输出的记录写出
ReduceTask1---->OutPutFormat（默认TextOutPutFormat）------>RecordWriter（LineRecoreWriter）
    LineRecoreWriter将一个key-value以一行写出，key和alue之间使用\t分割
     在输出目录中，生成文件part-r-0000
     hadoop	3
     hello	2
     hi	2
     hive	2
   
     ReduceTask2---->OutPutFormat（默认TextOutPutFormat）------>RecordWriter（LineRecoreWriter）
     LineRecoreWriter将一个key-value以一行写出，key和alue之间使用\t分割
     在输出目录中，生成文件part-r-0001
     spark	3
     wow	3
     zoo	3

#### MapReduce的编写

```java
1. Mapper
		MapTask中负责Map阶段核心运算逻辑的类！
		①继承Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
		②KEYIN,VALUEIN 取决于InputFormat中RecordReader的设置
			KEYOUT,VALUEOUT由自己定义
		③Mapper的运行流程
			由MapTask调用Mapper.run()
			
			run(){
				setUp();
				while(context.nextKeyValue()) { //循环调用RR读取下一组输入的key-value
					map(key,value,context);
				}
				cleanUp();
			}
		④在Mapper的map()中编写核心处理逻辑
				
2. Reducer
		ReduceTask中负责Reduce阶段核心运算逻辑的类！
		①继承Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
		②KEYIN,VALUEIN 取决于Mapper的输出
			KEYOUT,VALUEOUT由自己定义
		③Reducer的运行流程
			由MapTask调用Reducer.run()
			
			run(){
				setUp();
				while(context.nextKey()) { // 使用迭代器不断迭代key相同的数据
					reduce(key,value,context);
				}
				cleanUp();
			}
		④在Reducer的reduce()中编写核心处理逻辑

3. Job
		①创建Job
			Job job=Job.getInstance(Configuration conf);
			
		②设置Job
			Job.setName("job名");  //设置名称
			Job.setJar("jar名"); |  Job.setJarByClass(类名)  // 设置Job所在的Jar包，只有在YARN上运行时才需要设置
		③配置Job
				设置输入和输出格式，如果不设置使用默认
				设置Mapper,Reducer
				设置Mapper和Reducer的输出类型 // 主要为了方便根据类型获取对应的序列化器
				设置输入和输出目录
		④运行Job
				Job.waitforCompletion(true);
```





#### 常见的输入格式

1. **TextInputFormat**
   场景： TextInputFormat常用于输入目录中全部是文本文件！
   
   切片：默认的切片策略
   
   RecordReader:  LineRecordReader(将一行封装为一个key-value),一次处理一行，将一行内容的偏移量作为key，一行内容作为value!
LongWritable key(行的偏移量)
   Text value(行的内容)
   
2. **NlineInputFormat**
   
   场景：适合一行的内容特别多，在Map阶段map()处理的逻辑非常复杂！根据行数自定义切片的大小！

切片： 读取配置中mapreduce.input.lineinputformat.linespermap，默认为1，以文件为单位，切片每此参数行作为1片！

   RecordReader: LineRecordReader(将一行封装为一个key-value),一次处理一行，将一行内容的偏移量作为key，一行内容作为value!
   LongWritable key(行的偏移量)
   Text value(行的内容)

3. **KeyValueTextInputFormat**
    作用： 针对文本文件！使用分割字符，将每一行分割为key和value!

  如果没有找到分隔符，当前行的内容作为key，value为空串!
  默认分隔符为\t，可以通过参数mapreduce.input.keyvaluelinerecordreader.key.value.separator指定！
  切片：默认的切片策略
  RR ：  KeyValueLineRecordReader(将一行封装为一个key-value)
  Text key(行的分隔符之前的部分内容)
  Text value(行的分隔符之后的部分内容)

4. **ConbineTextInputFormat**
     作用： 改变了传统的切片方式！将多个小文件，划分到一个切片中！
       适合小文件过多的场景！
       RecordReader: LineRecordReader(将一行封装为一个key-value),一次处理一行，将一行内容的偏移量作为key，一行内容作为value!
       LongWritable key(行的偏移量)
       Text value(行的内容)
       切片：  先确定片的最大值maxSize，maxSize通过参数mapreduce.input.fileinputformat.split.maxsize设置！
       流程：
       a. 以文件为单位，将每个文件划分为若干part
       ①判断文件的待切部分的大小 <= maxSize,整个待切部分作为1part
       ②maxsize < 文件的待切部分的大小 <= 2* maxSize,将整个待切部分均分为2part
       ③文件的待切部分的大小 > 2* maxSize,先切去maxSize大小，作为1部分，剩余待切部分继续判断！
       举例： maxSize=2048
       t1.txt 4.38KB
       part1(t1.txt,0,2048)
       part2(t1.txt,2048,1219)
       part3(t1.txt.3xxx,1219)

       t2.txt 4.18KB
       part4(t2.txt,0,2048)
       part5(t2.txt,2048,1116)
       part6(t2.txt,3xxx,1116)

       t3.txt  2.71kb
       part7(t3.txt,0 ,13xxx)
       part8(t3.txt,13 ,27xx)

       t4.txt  5.04kb
       part9(t4.txt,0,2048)
       part10(t4.txt,2048,1519)
       part11(t4.txt.3xxx,1519)

b. 将之前切分的若干part进行累加，累加后一旦累加的大小超过 maxSize，这些作为1片！

#### Job提交流程

1. 提交之前的准备阶段
   - 检查输出目录是否合法
   - 为Job设置很多属性(用户，ip,主机名..)
   - 使用InputFormat对输入目录中的文件进行切片，设置Job运行的mapTask的数量为切片的数量
   - 在Job的作业目录生成Job执行的关键文件
      job.split (job的切片对象)
      job.splitmetainfo(job切片的属性信息)
      job.xml(job所有的配置)
   - 正式提交Job
2. 本地模式
		在提交Job后，创建LocalJobRunner.Job.Job对象，启动线程！
	在LocalJobRunner.Job.Job启动的线程中，使用线程池，用多线程的形式模拟MapTask和ReduceTask的多进程运行！
	执行Map,调用线程池，根据Map的切片信息，创建若干MapTaskRunable线程，在线程池上运行多个线程！
	MapTaskRunable------>MapTask--------->Mapper--------->Mapper.run()------->Mapper.map()
			
	Map阶段运行完后，执行Reduce,调用线程池，根据Job设置的ReduceTask的数量，
			创建若干ReduceTaskRunable线程，在线程池上运行多个线程！
			
	ReduceTaskRunable------->ReduceTask------>Reducer----->Reducer.run()------>Reducer.reduce()

3. YARN上运行
在提交Job后，创建MRAppMaster进程！
	由MRAppMaster，和RM申请，申请启动多个MapTask,多个ReduceTask
	Container------>MapTask--------->Mapper--------->Mapper.run()------->Mapper.map()
	Container------->ReduceTask------>Reducer----->Reducer.run()------>Reducer.reduce()
#### MapReduce总结

**Map阶段**(MapTask)：

1. 切片(Split)
2. 读取数据，封装为输入的k-v(Read)
3. 交给Mapper处理(Map)
4. 分区和排序(sort)

**Reduce阶段**(ReduceTask):  
1. 拷贝分区数据(copy)
2. 排序(sort)
3. 合并(reduce)
4. 写出(write)



## HBase

### 是什么

- **HBase**是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。
- **Hbase**面向列存储，构建于Hadoop之上，类似于Google的BigTable，提供对10亿级别表数据的快速随机实时读写！

### 作用

适合单表超千万，上亿，且高并发的情况！

### 组成

![HBase逻辑结构](C:\Users\halfofgame\Documents\MarkdownImages\HBase逻辑结构.png)

![HBase物理结构](C:\Users\halfofgame\Documents\MarkdownImages\HBase物理结构.png)

### 数据模型

1. **Name Space**
        命名空间，类似于关系型数据库的database概念，每个命名空间下有多个表。HBase两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间。
        一个表可以自由选择是否有命名空间，如果创建表的时候加上了命名空间后，这个表名字以<Namespace>:<Table>作为区分！

2. **Table**
       类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，数据属性，比如超时时间（TTL），压缩算法（COMPRESSION）等，都在列族的定义中定义，不需要声明具体的列。
       这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。

3. **Row**
	    HBase表中的每行数据都由一个RowKey和多个Column（列）组成。一个行包含了多个列，这些列通过列族来分类,行中的数据所属列族只能从该表所定义的列族中选取,不能定义这个表中不存在的列族，否则报错NoSuchColumnFamilyException。

4. **RowKey**
	    Rowkey由用户指定的一串不重复的字符串定义，是一行的唯一标识！数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。
	    如果使用了之前已经定义的RowKey，那么会将之前的数据更新掉！

5. **Column Family**
	    列族是多个列的集合。一个列族可以动态地灵活定义多个列。表的相关属性大部分都定义在列族上，同一个表里的不同列族可以有完全不同的属性配置，但是同一个列族内的所有列都会有相同的属性。
	列族存在的意义是HBase会把相同列族的列尽量放在同一台机器上，所以说，如果想让某几个列被放到一起，你就给他们定义相同的列族。
	    官方建议一张表的列族定义的越少越好，列族太多会极大程度地降低数据库性能，且目前版本Hbase的架构，容易出BUG。

6. **Column Qualifier**
	    Hbase中的列是可以随意定义的，一个行中的列不限名字、不限数量，只限定列族。因此列必须依赖于列族存在！列的名称前必须带着其所属的列族！例如info：name，info：age。
	因为HBase中的列全部都是灵活的，可以随便定义的，因此创建表的时候并不需要指定列！列只有在你插入第一条数据的时候才会生成。其他行有没有当前行相同的列是不确定，只有在扫描数据的时候才能得知！

7. **TimeStamp**
	    用于标识数据的不同版本（version）。时间戳默认由系统指定，也可以由用户显式指定。
	在读取单元格的数据时，版本号可以省略，如果不指定，Hbase默认会获取最后一个版本的数据返回！

8. **Cell**
	    一个列中可以存储多个版本的数据。而每个版本就称为一个单元格（Cell）。
	    Cell由{rowkey, column Family：column Qualifier, time Stamp}确定。
	    Cell中的数据是没有类型的，全部是字节码形式存贮。

![HBase-Cell](C:\Users\halfofgame\Documents\MarkdownImages\HBase-Cell.png)

 

9. **Region**
	    Region由一个表的若干行组成！在Region中行的排序按照行键（rowkey）字典排序。
	    Region不能跨RegionSever，且当数据量大的时候，HBase会拆分Region。
	    Region由RegionServer进程管理。HBase在进行负载均衡的时候，一个Region有可能会从当前RegionServer移动到其他RegionServer上。
	    Region是基于HDFS的，它的所有数据存取操作都是调用了HDFS的客户端接口来实现的。

### 基本架构



![HBase架构](C:\Users\halfofgame\Documents\MarkdownImages\HBase架构.png)

#### 架构角色

1. **Region Server**
	    RegionServer是一个服务，负责多个Region的管理。其实现类为HRegionServer，主要作用如下:
	    对于数据的操作：get, put, delete；
	    对于Region的操作：splitRegion、compactRegion。
	    客户端从ZooKeeper获取RegionServer的地址，从而调用相应的服务，获取数据。

2. **Master**
	    Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：
	    对于表的操作：create, delete, alter，这些操作可能需要跨多个                ReginServer，因此需要Master来进行协调！
    对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。
	    即使Master进程宕机，集群依然可以执行数据的读写，只是不能进行表的创建和修改等操作！当然Master也不能宕机太久，有很多必要的操作，比如创建表、修改列族配置，以及更重要的分割和合并都需要它的操作。

3. **Zookeeper**
	    RegionServer非常依赖ZooKeeper服务，ZooKeeper管理了HBase所有RegionServer的信息，包括具体的数据段存放在哪个RegionServer上。
	    客户端每次与HBase连接，其实都是先与ZooKeeper通信，查询出哪个RegionServer需要连接，然后再连接RegionServer。Zookeeper中记录了读取数据所需要的元数据表
	    hbase:meata,因此关闭Zookeeper后，客户端是无法实现读操作的！
	    HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。

4. **HDFS**
	    HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。

## Zookeeper

### 是什么

**Zookeeper**是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。

作用

![Zookeeper应用场景1](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper应用场景1.png)

![Zookeeper应用场景2](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper应用场景2.png)

![Zookeeper应用场景3](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper应用场景3.png)

![Zookeeper应用场景4](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper应用场景4.png)

![Zookeeper应用场景5](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper应用场景5.png)

### 数据结构

![Zookeeper数据结构](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper数据结构.png)

![Zookeeper节点类型](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper节点类型.png)

### 工作机制

![Zookeeper工作机制1](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper工作机制1.png)

![Zookeeper工作机制2](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper工作机制2.png)

### 监听器原理

![Zookeeper监听器原理](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper监听器原理.png)

### 选举机制

1）半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。

2）Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。

3）以一个简单的例子来说明整个选举的过程。

假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么，如图5-8所示。              

![Zookeeper选举机制](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper选举机制.png)

（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING；

（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING

（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；

（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；

（5）服务器5启动，同4一样当小弟。

### 写数据流程

![Zookeeper写数据流程](C:\Users\halfofgame\Documents\MarkdownImages\Zookeeper写数据流程.png)

## Hive

### 是什么

- **Hive**是由Facebook开源用于解决海量结构化日志的数据统计
- **Hive**是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。本质是：将HQL转化成MapReduce程序

![Hive工作流程](C:\Users\halfofgame\Documents\MarkdownImages\Hive工作流程.png)

1）Hive处理的数据存储在HDFS

2）Hive分析数据底层的实现是MapReduce

3）执行程序运行在Yarn上

### 架构原理

![Hive架构原理](C:\Users\halfofgame\Documents\MarkdownImages\Hive架构原理.png)

1．用户接口：Client

CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）

2．元数据：Metastore

元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；

默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore

3．Hadoop

使用HDFS进行存储，使用MapReduce进行计算。

4．驱动器：Driver

（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。

（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。

（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。

（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。

![Hive运行机制](C:\Users\halfofgame\Documents\MarkdownImages\Hive运行机制.png)

Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。



## Flume

### 是什么

**Flume**是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。

### 作用

![Flume的作用](C:\Users\halfofgame\Documents\MarkdownImages\Flume的作用.png)

### 基础架构

![Flume组成架构](C:\Users\halfofgame\Documents\MarkdownImages\Flume组成架构.png)

1) Agent

Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。

Agent主要有3个部分组成，Source、Channel、Sink。

2) Source

Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。

3) Sink

Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。

Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。

4) Channel

Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。

Flume自带两种Channel：Memory Channel和File Channel。

Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。

File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。

5) Event

传输单元，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。Event由**Header**和**Body**两部分组成，Header用来存放该event的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组。

![Flume_Event数据结构](C:\Users\halfofgame\Documents\MarkdownImages\Flume_Event数据结构.png)                          

6) Interceptors

​    在Flume中允许使用拦截器对传输中的event进行拦截和处理！拦截器必须实现org.apache.flume.interceptor.Interceptor接口。拦截器可以根据开发者的设定修改甚至删除event！Flume同时支持拦截器链，即由多个拦截器组合而成！通过指定拦截器链中拦截器的顺序，event将按照顺序依次被拦截器进行处理！

7) Channel Selectors

​    Channel Selectors用于source组件将event传输给多个channel的场景。常用的有replicating（默认）和multiplexing两种类型。replicating负责将event复制到多个channel，而multiplexing则根据event的属性和配置的参数进行匹配，匹配成功则发送到指定的channel!

8) Sink Processors

​    用户可以将多个sink组成一个整体（sink组），Sink Processors可用于提供组内的所有sink的负载平衡功能，或在时间故障的情况下实现从一个sink到另一个sink的故障转移。

## Kafka

### 是什么

**Kafka**是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)称为broker。无论是kafka集群，还是consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。

### 应用场景(和不用消息队列对比)

![消息队列的作用](C:\Users\halfofgame\Documents\MarkdownImages\消息队列的作用.png)

1. 需要在多个应用和系统间提供高可靠的实时数据通道

2. 一些需要实时传输数据及及时计算的应用

### 作用

- 类似于消息队列和商业的消息系统，kafka提供对流式数据的发布和订阅

- kafka提供一种持久的容错的方式存储流式数据

- kafka拥有良好的性能，可以及时地处理流式数据

### 消息队列的两种模式

1. **点对点模式**（一对一，消费者主动拉取数据，消息收到后消息清除）

   消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。

   消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。

   ![Kafka_点对点模式](C:\Users\halfofgame\Documents\MarkdownImages\Kafka_点对点模式.png)

2. **发布和订阅模式**（一对多，消费者消费数据之后不会清除消息）

   消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。

   ![Kafka_发布订阅模式](C:\Users\halfofgame\Documents\MarkdownImages\Kafka_发布订阅模式.png)

### 核心概念

**1.Broker**
    一台kafka服务器就是一个broker。一个集群由多个broker组成。



**2.Topic**

    Topic 就是数据主题，kafka建议根据业务系统将不同的数据存放在不同的topic中！Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。一个大的Topic可以分布式存储在多个kafka broker中！Topic可以类比为数据库中的库！



**3.Partition**

    每个topic可以有多个分区，通过分区的设计，topic可以不断进行扩展！即一个Topic的多个分区分布式存储在多个broker!

此外通过分区还可以让一个topic被多个consumer进行消费！以达到并行处理！分区可以类比为数据库中的表！

kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。



**4.Offset**

     数据会按照时间顺序被不断第追加到分区的一个结构化的commit log中！每个分区中存储的记录都是有序的，且顺序不可变！

这个顺序是通过一个称之为offset的id来唯一标识！因此也可以认为offset是有序且不可变的！ 

在每一个消费者端，会唯一保存的元数据是offset（偏移量）,即消费在log中的位置.偏移量由消费者所控制。通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从"现在"开始消费。

这些细节说明Kafka 消费者是非常廉价的—消费者的增加和减少，对集群或者其他消费者没有多大的影响。比如，你可以使用命令行工具，对一些topic内容执行 tail操作，并不会影响已存在的消费者消费数据。

 

**5.持久化**

    Kafka 集群保留所有发布的记录—无论他们是否已被消费—并通过一个可配置的参数——保留期限来控制。举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被清除并释放磁盘空间。
    
    Kafka的性能和数据大小无关，所以长时间存储数据没有什么问题。

**6.副本机制**

    日志的分区partition （分布）在Kafka集群的服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性。
    
    每个分区都有一台 server 作为 “leader”，零台或者多台server作为 follwers 。leader server 处理一切对 partition （分区）的读写请求，而follwers只需被动的同步leader上的数据。当leader宕机了，followers 中的一台服务器会自动成为新的 leader。通过这种机制，既可以保证数据有多个副本，也实现了一个高可用的机制！
    
    基于安全考虑，每个分区的Leader和follower一般会错在在不同的broker!



**7.Producer**

消息生产者，就是向kafka broker发消息的客户端。生产者负责将记录分配到topic的指定 partition（分区）中

 

**8.Consumer**

    消息消费者，向kafka broker取消息的客户端。每个消费者都要维护自己读取数据的offset。低版本0.9之前将offset保存在Zookeeper中，0.9及之后保存在Kafka的“__consumer_offsets”主题中。

 


**9.Consumer Group** 

    每个消费者都会使用一个消费组名称来进行标识。同一个组中的不同的消费者实例，可以分布在多个进程或多个机器上！

如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例（单播）。即每个消费者可以同时读取一个topic的不同分区！

如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程(广播)。

如果需要实现广播，只要每个consumer有一个独立的组就可以了。要实现单播只要所有的consumer在同一个组。

一个topic可以有多个consumer group。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。

### 基础架构

![Kafka基础架构](C:\Users\halfofgame\Documents\MarkdownImages\Kafka基础架构.png)

## Phoniex

### 是什么

​    **Phoenix** 是构建在 HBase 之上的开源 SQL 层. 能够让我们使用标准的 JDBC API 去建表, 插入数据和查询 HBase 中的数据, 从而可以避免使用 HBase 的客户端 API.
​    在我们的应用和 HBase 之间添加了 Phoenix, 并不会降低性能, 而且我们也少写了很多代码.



## Sqoop

### 是什么

**Sqoop**是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、oracle等)之间进行数据的传递，可以将一个关系型数据库（MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。



